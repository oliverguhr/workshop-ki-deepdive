{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstieg in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,1])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[1,2],[1,2]])\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views und Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2],[1,2],[1,2]]) @ torch.tensor([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# standard plotting library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.linspace(-5, 5, 1000)\n",
    "plt.plot(prob, torch.relu(prob))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lossfunktionen\n",
    "\n",
    "Eine Lossfunktion, auch Verlust- oder Kostenfunktion genannt, bewertet die Leistung eines maschinellen Lernmodells, indem sie die Abweichung zwischen den Vorhersagen des Modells und den tatsächlichen Zielen misst. Diese Funktion gibt einen numerischen Wert aus, der minimiert werden soll, und bietet damit ein quantitatives Maß, das im Trainingsprozess zur Optimierung des Modells verwendet wird.\n",
    "\n",
    "\n",
    "### Regression: Mean Absolut Error oder L1 Loss\n",
    "\n",
    "Dieses Loss wird genuzt wenn das Netz einen konkreten Wert vorhersagen soll. Beispielsweise den zukünftigen Pegelstand der Elbe auf basis der historischen Pegelstände."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([1.0]) # Ausgabe des Netzwerks\n",
    "target = torch.tensor([5.0]) # Zielwert\n",
    "\n",
    "absoult_error = torch.abs(output - target)\n",
    "\n",
    "print(\"absolut error\", absoult_error)\n",
    "\n",
    "mean_absolut_error = torch.mean(absoult_error)\n",
    "\n",
    "print(\"mean absolut error\", mean_absolut_error)\n",
    "\n",
    "print(\"l1 loss == mean absolut error\",mean_absolut_error, F.l1_loss(output,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klassifikation: Softmax und Cross Entropy Loss\n",
    "\n",
    "\n",
    "Dieses Loss wird genutzt wenn das Netz eine Eingabe einer konkreten Klasse zuordnen soll. \n",
    "Zum Beispiel wenn das Netz entscheiden soll ob das einen Hund oder eine Katze darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Unser Netz kann beliebge Werte ausgeben\n",
    "output = torch.tensor([ 0.0556, -0.0389, 0.6144])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Softmax-Funktion wandelt einen Vektor von K reellen Zahlen in eine Wahrscheinlichkeitsverteilung von K möglichen Ergebnissen um.\n",
    "\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\ \\ \\ \\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf z=(z_1,\\dotsc,z_K) \\in\\R^K.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist = torch.softmax(output, dim=0)\n",
    "prob_dist\n",
    "\n",
    "print(prob_dist)\n",
    "print(torch.sum(prob_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_index = torch.argmax(prob_dist)\n",
    "\n",
    "print(most_likely_index)\n",
    "print(prob_dist[most_likely_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Das negative log likelihood loss\n",
    "\n",
    "\n",
    "Der Logarithmus transformiert Wahrscheinlichkeiten in eine Lossfunktion. Je niedriger die Wahrscheinlichkeit, die ein Modell der richtigen Klasse zuweist, desto höher ist der resultierende Loss. Das passt gut zu Optimierungsalgorithmen, die versuchen, das Loss zu minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.linspace(0.001, 1, 1000)\n",
    "plt.plot(prob, -torch.log(prob))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -torch.log(torch.tensor(0.01)) # wahrscheinlichkeit von 10%\n",
    "print(\"Loss bei einer Wahrscheinlichkeit von 1%:\\t\",loss.item())\n",
    "\n",
    "loss = -torch.log(torch.tensor(0.50)) # wahrscheinlichkeit von 80%\n",
    "print(\"Loss bei einer Wahrscheinlichkeit von 50%:\\t\",loss.item())\n",
    "\n",
    "loss = -torch.log(torch.tensor(0.99)) # wahrscheinlichkeit von 80%\n",
    "print(\"Loss bei einer Wahrscheinlichkeit von 99%:\\t\",loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "Die Kombination aus Softmax und NNL Loss nennt sich auch \"cross entropy\" loss.\n",
    "\n",
    "\n",
    "Es kombiniert zwei Operationen:\n",
    "\n",
    "1. Umwandel der Ausgaben des Netzes in eine Wahrscheinlichkeitsverteilung \n",
    "2. Skalieren des Losses über den negativen Logaritmus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([ 0.0556, -0.0389, 0.6144])\n",
    "F.cross_entropy(output,torch.tensor(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wie lernt das Netz?\n",
    "\n",
    "Wir nutzen die Ableitung der Fehlerfunktio, auch Fehlergradient genant, um die Gewichte des Netzes zu optimieren. Das Verfahren nennt sich Gradientenabstieg oder gradient descent.  \n",
    "\n",
    "![Backpropagation](data/backpropagation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([100,0.0]) \n",
    "w = torch.tensor([0.009, 0.5], requires_grad=True)\n",
    "y = torch.Tensor([1.0])\n",
    "\n",
    "prediction = x @ w\n",
    "print(\"Vorhersage vor dem Lernschritt\",prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(prediction,y[0])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "with torch.no_grad():\n",
    "    w -= (learning_rate * w.grad)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction  = F.sigmoid(x @ w)\n",
    "print(\"Vorhersage nach dem Lernschritt\",prediction.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Verfahren kann noch verbessert werden z.B. in dem wir einen Term für [das Momentum einführen](https://distill.pub/2017/momentum/).\n",
    "Des weiteren gibt es eine Reihe von verschiedenen [Opimierungsverfahren zur Auswahl](https://pytorch.org/docs/stable/optim.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
